{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, csv, itertools, glob\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     19,
     24,
     30
    ]
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def load_pickle(filename):\n",
    "    try:\n",
    "        p = open(filename, 'r')\n",
    "    except IOError:\n",
    "        print(\"Pickle file cannot be opened.\")\n",
    "        return None\n",
    "    try:\n",
    "        picklelicious = pk.load(p)\n",
    "    except ValueError:\n",
    "        print('load_pickle failed once, trying again')\n",
    "        p.close()\n",
    "        p = open(filename, 'r')\n",
    "        picklelicious = pk.load(p)\n",
    "\n",
    "    p.close()\n",
    "    return picklelicious\n",
    "\n",
    "def save_pickle(data_object, filename):\n",
    "    pickle_file = open(filename, 'w')\n",
    "    pk.dump(data_object, pickle_file)\n",
    "    pickle_file.close()\n",
    "    \n",
    "def read_data(filename):\n",
    "    print(\"Loading Data...\")\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    data = df.values\n",
    "    return data\n",
    "\n",
    "def read_line(csvfile, line):\n",
    "    with open(csvfile, 'r') as f:\n",
    "        data = next(itertools.islice(csv.reader(f), line, None))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     42,
     57,
     64,
     68,
     72,
     78,
     126,
     158,
     195,
     258,
     267,
     276,
     285,
     294
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#  Classifiers: Code directly modified based on the PyTorch Model Zoo Implementations\n",
    "#####################################################################################################################\n",
    "\n",
    "## 1D variant of VGG model take 200 dimensional fixed time series inputs\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes, arch='vgg'):\n",
    "        super(VGG, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 6, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool1d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv1d = nn.Conv1d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv1d, nn.BatchNorm1d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv1d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def vgg13bn(**kwargs):\n",
    "    model = VGG(make_layers(cfg['B'], batch_norm=True), arch='vgg13bn', **kwargs)\n",
    "    return model\n",
    "\n",
    "def vgg16bn(**kwargs):\n",
    "    model = VGG(make_layers(cfg['D'], batch_norm=True), arch='vgg16bn', **kwargs)\n",
    "    return model\n",
    "\n",
    "def vgg19bn(**kwargs):\n",
    "    model = VGG(make_layers(cfg['E'], batch_norm=True), arch='vgg19bn', **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "## Multilayer LSTM based classifier taking in 200 dimensional fixed time series inputs\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.arch = 'lstm'\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_dir = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "                input_size=in_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "\n",
    "        self.hidden2label = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*self.num_dir, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "        # self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if cuda:\n",
    "            h0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim).cuda())\n",
    "            c0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, x): # x is (batch_size, 1, 200), permute to (200, batch_size, 1)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # See: https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/2\n",
    "        lstm_out, (h, c) = self.lstm(x, self.init_hidden())\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        return y\n",
    "    \n",
    "\n",
    "## 1D Variant of ResNet taking in 200 dimensional fixed time series inputs\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        # print('out', out.size(), 'res', residual.size(), self.downsample)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=1, padding=1, stride=stride, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, padding=1, stride=stride, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, arch):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1]) #, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2]) #, stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3]) #, stride=2)\n",
    "        self.avgpool = nn.AvgPool1d(7, stride=1)\n",
    "        self.fc = nn.Linear(22528 , num_classes) # 512 * block.expansion\n",
    "        \n",
    "        self.arch = arch\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.size())\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], arch='resnet18', **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], arch='resnet34', **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], arch='resnet50', **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], arch='resnet101', **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], arch='resnet152', **kwargs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#  DataLoaders\n",
    "#####################################################################################################################\n",
    "\n",
    "# Dummy Dataset for testing purpose only\n",
    "class DummyDataset(Dataset):\n",
    "    \"\"\"Time Series dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, numclasses=15):\n",
    "        self.numclasses = numclasses\n",
    "\n",
    "    def __len__(self):\n",
    "        return 512\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.random.randn(200)\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "\n",
    "        return data, np.random.randint(self.numclasses)\n",
    "    \n",
    "# Synchronized Time Series, allow variable training window\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Time Series dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, norm, is_train, train_window):\n",
    "\n",
    "        self.file_dirs = sorted([f for f in glob.glob('%s/*/*/*'%data_path) if os.path.isdir(f)])\n",
    "        self.norm = norm\n",
    "        self.is_train = is_train\n",
    "        self.train_window = train_window\n",
    "        self.files_per_dir = self.train_window**2 if self.is_train else (32**2 - self.train_window**2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int( len(self.file_dirs) * self.files_per_dir )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fdir = self.file_dirs[ idx // self.files_per_dir ] \n",
    "        file_id = idx % self.files_per_dir + ( 0 if self.is_train else (self.train_window**2) )\n",
    "        data = np.fromfile(os.path.join(fdir, '%s.bin'%str(file_id).zfill(4)))\n",
    "        target = int(fdir.split('/')[3]) # Be careful about this index !!\n",
    "        \n",
    "        if self.norm:\n",
    "            data -= data.mean()\n",
    "            data /= data.std()\n",
    "        else:\n",
    "            data -= data[0]\n",
    "        \n",
    "        data = np.expand_dims(data, axis=0)\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     138
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#  Trainer and Core Experiment Scripts\n",
    "#####################################################################################################################\n",
    "\n",
    "def run_trainer(experiment_path, model_path, model, train_loader, test_loader, get_acc, resume, batch_size, num_epoch):\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    def save_checkpoint(state, is_best, filename=model_path+'checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, model_path+'model_best.pth.tar')\n",
    "    def get_last_checkpoint(model_path):\n",
    "        fs = sorted([f for f in os.listdir(model_path) if 'Epoch' in f], key=lambda k: int(k.split()[1]))\n",
    "        return model_path+fs[-1] if len(fs) > 0 else None\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_res = 0\n",
    "    lrcurve = []\n",
    "    conf_mats = []\n",
    "    resume_state = get_last_checkpoint(model_path) if resume else None\n",
    "    if resume_state and os.path.isfile(resume_state):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume_state))\n",
    "        checkpoint = torch.load(resume_state)\n",
    "        start_epoch = checkpoint['epoch']+1\n",
    "        best_res = checkpoint['val_acc']\n",
    "        lrcurve = checkpoint['lrcurve']\n",
    "        conf_mats = checkpoint['conf_mats']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resume_state, checkpoint['epoch']))\n",
    "    else:\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5) # optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5)\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        total, total_correct = 0., 0.\n",
    "        train_conf_mats = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data.float()), Variable(target.long())\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, correct, num_instance, conf_mat = get_acc(output, target)\n",
    "            total_correct += correct\n",
    "            total += num_instance\n",
    "            train_conf_mats.append(conf_mat)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Acc: {:.2f}%/{:.2f}%'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0],\n",
    "                    100. * correct / num_instance, 100. * total_correct / total ))\n",
    "        \n",
    "        return 100. * total_correct / total, np.dstack(train_conf_mats).sum(axis=2)\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total, total_correct = 0., 0.\n",
    "        test_conf_mats = []\n",
    "        preds = []\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.long())\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).data[0] # sum up batch loss\n",
    "            \n",
    "            pred, correct, num_instance, conf_mat = get_acc(output, target)\n",
    "            total_correct += correct\n",
    "            total += num_instance\n",
    "            test_conf_mats.append(conf_mat)\n",
    "            preds.append(pred)\n",
    "\n",
    "        test_acc = 100. * total_correct / total\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, total_correct, total,\n",
    "            test_acc))\n",
    "\n",
    "        return np.vstack(preds), test_acc, np.dstack(test_conf_mats).sum(axis=2)\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epoch):\n",
    "        is_best = False\n",
    "\n",
    "        train_acc, train_conf = train(epoch)\n",
    "        preds, val_acc, val_conf = test()\n",
    "        \n",
    "        print(\"Training Confmat: \")\n",
    "        print(train_conf)\n",
    "        print(\"Testing Confmat: \")\n",
    "        print(val_conf)\n",
    "        print(\"Number of Predictions Made: \")\n",
    "        print(preds.shape)\n",
    "        \n",
    "        lrcurve.append((train_acc, val_acc))\n",
    "        conf_mats.append((train_conf, val_conf))\n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "        if val_acc > best_res:\n",
    "            best_res = val_acc\n",
    "            is_best = True\n",
    "\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'arch': model.arch,\n",
    "                'state_dict': model.cpu().state_dict(),\n",
    "                'train_acc':train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'lrcurve':lrcurve,\n",
    "                'train_conf':train_conf,\n",
    "                'val_conf':val_conf,\n",
    "                'conf_mats':conf_mats,\n",
    "                'test_predictions':preds,\n",
    "            }, is_best,\n",
    "            model_path+\"Epoch %d Acc %.4f.pt\"%(epoch, val_acc))\n",
    "\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "            \n",
    "    return lrcurve, conf_mats\n",
    "\n",
    "def run_dummy_experiment(experiment_path, model_root, models, norm, train_window, get_acc, resume=False, num_epoch=10):\n",
    "    \n",
    "    exp_result = {}\n",
    "    for batch_size, model in models:\n",
    "        print(\"Running %s\" % model.arch)\n",
    "        \n",
    "        print('Loading Data..')\n",
    "        train_data = DummyDataset()\n",
    "        test_data = DummyDataset()\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "    \n",
    "        model_path = os.path.join(\n",
    "            experiment_path, \n",
    "            model_root,\n",
    "            'norm' if norm else 'nonorm',\n",
    "            str(train_window),\n",
    "            model.arch) + '/'\n",
    "        lrcurve, conf_mats = run_trainer(experiment_path, model_path, model, train_loader, test_loader, get_acc, resume, batch_size, num_epoch)\n",
    "        exp_result[model.arch] = {'lrcurve':lrcurve, 'conf_mats':conf_mats}\n",
    "        \n",
    "    return exp_result\n",
    "        \n",
    "def run_experiment(experiment_path, data_path, model_root, models, norm, train_window, get_acc, resume=False, num_epoch=10):\n",
    "    \n",
    "    exp_result = {}\n",
    "    for batch_size, model in models:\n",
    "        print(\"Running %s\" % model.arch)\n",
    "        \n",
    "        print('Loading Data..')\n",
    "        train_data = TimeSeriesDataset(data_path, norm, True, train_window)\n",
    "        test_data = TimeSeriesDataset(data_path, norm, False, train_window)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "        model_path = os.path.join(\n",
    "            experiment_path, \n",
    "            model_root,\n",
    "            'norm' if norm else 'nonorm',\n",
    "            str(train_window),\n",
    "            model.arch) + '/'\n",
    "        lrcurve, conf_mats = run_trainer(experiment_path, model_path, model, train_loader, test_loader, get_acc, resume, batch_size, num_epoch)\n",
    "        exp_result[model.arch] = {'lrcurve':lrcurve, 'conf_mats':conf_mats}\n",
    "        \n",
    "    return exp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#  Dummy Experiment for Testing Purpose\n",
    "#####################################################################################################################\n",
    "\n",
    "def get_models():\n",
    "    return [\n",
    "        (1024, vgg13bn(num_classes=15)),\n",
    "        (256, LSTMClassifier(\n",
    "            in_dim=1,\n",
    "            hidden_dim=120,\n",
    "            num_layers=3,\n",
    "            dropout=0.8,\n",
    "            bidirectional=True,\n",
    "            num_classes=15,\n",
    "            batch_size=256\n",
    "        )),\n",
    "        (1024, resnet34(num_classes=15))\n",
    "    ]\n",
    "\n",
    "def get_acc(output, target):\n",
    "    # takes in two tensors to compute accuracy\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    conf_mat = confusion_matrix(pred.cpu().numpy(), target.data.cpu().numpy(), labels=range(15))\n",
    "    return pred.cpu().numpy(), correct, target.size(0), conf_mat\n",
    "\n",
    "dummy_experiment = {\n",
    "    'experiment_path':'dummy', \n",
    "    'model_root':'model', \n",
    "    'models':get_models(), \n",
    "    'norm':False, \n",
    "    'train_window':24, \n",
    "    'get_acc': get_acc,\n",
    "    'resume':False, \n",
    "    'num_epoch':3\n",
    "}\n",
    "\n",
    "# exp_log = run_dummy_experiment(**experiment)\n",
    "# print(exp_log)\n",
    "# experiment_filepath = os.path.join(\n",
    "#                         experiment['experiment_path'], \n",
    "#                         experiment['model_root'],\n",
    "#                         'norm' if experiment['norm'] else 'nonorm',\n",
    "#                         str(experiment['train_window']),\n",
    "#                         'exp_log.pkl')\n",
    "# save_pickle(exp_log, experiment_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#  Material Recognition Experiment with Deep Models\n",
    "#####################################################################################################################\n",
    "\n",
    "def get_models(): # tuples of (batch_size, model)\n",
    "    return [\n",
    "        (1024, vgg13bn(num_classes=15)),\n",
    "        (256, LSTMClassifier(\n",
    "            in_dim=1,\n",
    "            hidden_dim=120,\n",
    "            num_layers=3,\n",
    "            dropout=0.8,\n",
    "            bidirectional=True,\n",
    "            num_classes=15,\n",
    "            batch_size=256\n",
    "        )),\n",
    "        (1024, resnet34(num_classes=15))\n",
    "    ]\n",
    "\n",
    "def get_acc(output, target):\n",
    "    # takes in two tensors to compute accuracy\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    conf_mat = confusion_matrix(pred.cpu().numpy(), target.data.cpu().numpy(), labels=range(15))\n",
    "    return np.squeeze(pred.cpu().numpy()), correct, target.size(0), conf_mat\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        'experiment_path':'roll', \n",
    "        'data_path':'roll/roll',\n",
    "        'model_root':'model', \n",
    "        'models':get_models(),\n",
    "        'norm':False, \n",
    "        'train_window':24, \n",
    "        'get_acc': get_acc,\n",
    "        'resume':True,  \n",
    "        'num_epoch':10\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'experiment_path':'roll', \n",
    "        'data_path':'roll/roll',\n",
    "        'model_root':'model', \n",
    "        'models':get_models(),\n",
    "        'norm':True, \n",
    "        'train_window':24, \n",
    "        'get_acc': get_acc,\n",
    "        'resume':True, \n",
    "        'num_epoch':10\n",
    "    },\n",
    "]\n",
    "\n",
    "for experiment in experiments:\n",
    "    exp_log = run_experiment(**experiment)\n",
    "    print(exp_log)\n",
    "    experiment_filepath = os.path.join(\n",
    "                        experiment['experiment_path'], \n",
    "                        experiment['model_root'],\n",
    "                        'norm' if experiment['norm'] else 'nonorm',\n",
    "                        str(experiment['train_window']),\n",
    "                        'exp_log.pkl')\n",
    "    save_pickle(exp_log, experiment_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
